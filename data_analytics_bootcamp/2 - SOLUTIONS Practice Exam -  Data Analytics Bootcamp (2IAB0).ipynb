{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions Practice Exam -  Data Analytics Bootcamp (2IAB0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saturday 14th of April 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practice exam for the course Data Analytics for Engineers (2017/2018) consists of 23 open-ended questions. These questions have been primarily selected based on exam topics students indicated they found most challenging: \n",
    "![dae_topics](../img/self_reported_understanding_dae_topics_small.png)\n",
    "\n",
    "Note that the actual exam will consist of multiple choice questions which cover the material from *all* lectures.  \n",
    "Good luck with your final exam preparations!\n",
    "\n",
    "**Roy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the following relational model for a president database. Formulate the necessary queries to answer the four question below. You don't need any database or Python for that; paper works fine (but I will guarantee you: it is at least if not more challenging!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![relational_model](../img/ER_model_presidents.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1: How many children were born in marriages before 1900 and from 1950 till today, where the spouse is in her 30s at her marriage? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just to be clear: I don't expect a number here but the SQL-query that would give the desired number. As an aid, have a look at some fictitious data below. Note that, `NR_CHILDREN` indicates the number of children at the moment of the marriage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| PRES_ID | SPOUSE_NAME | SPOUSE_AGE | NR_CHILDREN | MARRIAGE_YEAR | \n",
    "| :------- | :------- | :------- | :------- | :------- | :------- | \n",
    "| 1 | Adriana Smith | 31 | 2 | 1890 | \n",
    "| 2 | Marjan Skelton | 29 | 0 | 1897 | \n",
    "| 3 | Eva Kortright | 30 | 1 | 1903 | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step-by-step solution:***\n",
    "```sql\n",
    "-- before 1900 and from 1950 till today\n",
    "SELECT * \n",
    "FROM pres_marriage\n",
    "WHERE marriage_year <1900 OR marriage_year >=1950;\n",
    "```\n",
    "```sql\n",
    "-- where the spouse is in her 30s at her marriage\n",
    "SELECT *\n",
    "FROM PRES_MARRIAGE\n",
    "WHERE (marriage_year >=1950 OR marriage_year <1900) AND spouse_age BETWEEN 30 AND 39; -- why are the parentheses required in this case?! \n",
    "```\n",
    "```sql\n",
    "-- how many children \n",
    "SELECT SUM(nr_children)\n",
    "FROM PRES_MARRIAGE\n",
    "WHERE (marriage_year >=1950 OR marriage_year <1900) AND spouse_age BETWEEN 30 AND 39;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**1.2: Determine the maximum difference between the youngest died and oldest died president that served more than 4 years. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step-by-step solution:***\n",
    "```sql\n",
    "-- that served more than 4 years\n",
    "SELECT *\n",
    "FROM president\n",
    "WHERE years_served > 4;\n",
    "```\n",
    "```sql \n",
    "-- the maximum difference between the youngest died and oldest died president\n",
    "SELECT MAX(death_age) - MIN(death_age)\n",
    "FROM president\n",
    "WHERE years_served > 4;\n",
    "```\n",
    "```sql\n",
    "-- add column name (optional)\n",
    "SELECT MAX(death_age) - MIN(death_age) AS difference\n",
    "FROM president\n",
    "WHERE years_served > 4;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Select the number of marriages in which a child was born on an average of minimal once in 5 years (starting with the year the wedding took place in, up to the year the spouse reached age 45).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step-by-step solution:***\n",
    "```sql\n",
    "-- average of minimal 1 child per 5 years from the marriage year to the age of 45\n",
    "SELECT * \n",
    "FROM pres_marriage \n",
    "WHERE nr_children >= (45-spouse_age) / 5.0; -- notice the decimal (i.e. 5.0)!\n",
    "```\n",
    "```sql\n",
    "-- make sure the WHERE-clause always works as expected\n",
    "SELECT * \n",
    "FROM pres_marriage \n",
    "WHERE nr_children >= (45-spouse_age) / 5.0 AND spouse_age <= 45;\n",
    "```\n",
    "```sql\n",
    "-- select the number of marriages\n",
    "SELECT COUNT(*)\n",
    "FROM pres_marriage \n",
    "WHERE nr_children >= (45-spouse_age) / 5.0 AND spouse_age <= 45;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 Determine for each president married more than once his id and the greatest as well as the least number of children born in his marriages (see example below).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| PRES_ID | MAX_CHILDREN | MIN_CHILDREN | \n",
    "| :-----  | :----------- | :----------- |\n",
    "| 10 | 5 | 2| \n",
    "| 13 | 3 | 0 | \n",
    "| 28 | 2 | 1 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Step-by-step solution:***\n",
    "```sql\n",
    "-- how many marriages for each president? \n",
    "SELECT pres_id, COUNT(pres_id) \n",
    "FROM pres_marriage\n",
    "GROUP BY pres_id;\n",
    "```\n",
    "```sql\n",
    "-- married more than once \n",
    "SELECT pres_id, COUNT(pres_id) \n",
    "FROM pres_marriage\n",
    "GROUP BY pres_id\n",
    "HAVING COUNT(pres_id) > 1;\n",
    "```\n",
    "```sql\n",
    "-- greatest number of children born\n",
    "SELECT pres_id, MAX(nr_children)\n",
    "FROM pres_marriage\n",
    "GROUP BY pres_id\n",
    "HAVING COUNT(pres_id) > 1;\n",
    "```\n",
    "```sql\n",
    "-- least number of children born\n",
    "SELECT pres_id, MAX(nr_children), MIN(nr_children)\n",
    "FROM pres_marriage\n",
    "GROUP BY pres_id\n",
    "HAVING COUNT(pres_id) > 1;\n",
    "```\n",
    "```sql\n",
    "-- (optional) ORDER BY pres_id \n",
    "SELECT pres_id, MAX(nr_children) AS max_children, MIN(nr_children) AS min_children -- MAX_CHILDREN and MIN_CHILDREN are fine too of course!\n",
    "FROM pres_marriage\n",
    "GROUP BY pres_id\n",
    "HAVING COUNT(pres_id) > 1\n",
    "ORDER BY pres_id;\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we want to determine whether men and women have *different* average salaries. Students are asked to write the corresponding null ($H_0$) and alternative hypothesis ($H_a$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 One of the students formulates the hypotheses as follows. What two critical mistakes did the student make? What should the hypotheses be instead?**   \n",
    "<br/>\n",
    "$H_0$:  $\\bar{X}_{male} = \\bar{X}_{female}$   \n",
    "$H_a$:  $\\bar{X}_{male} > \\bar{X}_{female}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "1. It should be a *two-sided* rather than an *one-sided* hypothesis test. After all, it could well be the case that women are making more money than men (despite the glass ceiling).\n",
    "2. $\\bar{X}$ should be replaced by $\\mu$ because hypotheses are about *parameters* rather than *statistics* (see [this video](https://youtu.be/tTeMYuS87oU?t=6m27s) for more details).\n",
    "\n",
    "Incorporating both changes gives the following hypotheses:  \n",
    "$H_0$:  $\\mu_{male} = \\mu_{female}$    \n",
    "$H_a$:  $\\mu_{male} \\neq \\mu_{female}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 After conducting the right hypotheses tests, the student finds a p-value of 0.19 and thus concludes that there is strong evidence that men earn as much as women. Is this statement correct? If so/not, why? (you can assume a significance level of 0.05)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "Given the p-value of 0.19 and a significance level of 0.05 the student rightfully accepts the null-hypothesis ($H_0$). That is to say, there is little to no evidence that the true mean salary for male ($\\mu_{male}$) differs from women ($\\mu_{female}$). However, that does *not* automatically imply that the salaries are equal to each other. See [this video](https://www.youtube.com/watch?v=Xi33dGcZCA0&list=PLvxOuBpazmsNo893xlpXNfMzVpRBjDH67) for a similar example (watch until the end to find the answer!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 What's the effect on the rejection region if we go from a two-sided to a one-sided hypothesis test and the significance level remains the same (e.g. 0.05). What does that imply?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "The rejection region on one side increases because the critical value will be closer to the centre of the distribution (i.e. 0 for a Z-distribution). In other words, we are more likely to reject the null hypothesis ($H_0$) and thus accept the alternative hypothesis ($H_a$). See [this video](https://youtu.be/VP1bhopNP74?t=2m3s) for a great visual representation and explanation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Given the hype around data analytics and data scientist being the [sexiest job](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century) of the 21st century, Glassdoor claims the average salary for data science roles in San Fransisco is [143K USD](https://www.glassdoor.nl/Salarissen/san-francisco-data-scientist-salarissen-SRCH_IL.0,13_IM759_KO14,28.htm?countryRedirect=true). You, a Data Analytics student, suspect this claim and therefore test the following hypotheses:**\n",
    "\n",
    "$H_{0}$:  $\\mu_{salary} = 143000\\ USD$    \n",
    "$H_{a}$:  $\\mu_{salary} < 143000\\ USD$ \n",
    "\n",
    "**Despite the fact that you find a sample mean ($\\bar{X}$) of \\$142895 - very close to the hypothesized mean ($\\mu_{salary}$) - you decide to accept the alternative hypothesis ($H_a$) because of a p-value of 0.004. How can you explain this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "Statistical significance is strongly related to sample size. So say that you gathered the data from thousands of data scientists relatively small salary differences can still be statistically significant. [This video](https://www.youtube.com/watch?v=_k1MQTUCXmU&list=PLvxOuBpazmsNo893xlpXNfMzVpRBjDH67&index=8) provides a similar example related to this titled \"Statistical Significance versus Practical Significance\".  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 In hypotheses testing we distinguish the Z-test and T-test. Explain when to use which one. Which one do you think is more common?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "The formulas for both tests are as follows:  \n",
    "$Z = \\frac{\\bar{X}-\\mu_{0}}{\\sigma / \\sqrt{n}}$ and $t = \\frac{\\bar{X}-\\mu_{0}}{s / \\sqrt{n}}$  \n",
    "The Z-test requires us to know the value for $\\sigma$ which is the standard deviation for the **population** (while $s$ is the standard deviation for the **sample**). In practice it's very hard to obtain the value for $\\sigma$ because it means you have to gather data from everyone within your population (e.g. all employed men and women worldwide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 What's the effect on the rejection region if the sample size increases for a Z-test and T-test.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "See the formulas above. If $n$ increases, $\\frac{s}{\\sqrt{n}}$ and $\\frac{\\sigma}{\\sqrt{n}}$ become smaller. Since the denominator decreases both the Z and t values will increase. In that sense, it's \"easier\" to reject the null hypothesis. \n",
    "\n",
    "For the t-test there is an additional effect we should consider because the critical value is dependent on the sample size. As you can see below for a given significance level ($\\alpha$) this critical value decreases as the sample size (denoted by $\\nu$) increases. For example, $t_{4; 0.1}$ is smaller than $t_{3; 0.1}$. As such, for a t-test it is again \"easier\" to reject the null hypothesis and thus the rejection region increases. \n",
    "\n",
    "![t_table](../img/t_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Type I and II errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a criminal trial. We test the hypotheses:   \n",
    "\n",
    "$H_0$: The defendant did not commit the crime.   \n",
    "$H_a$: The defendant committed the crime.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 Explain in your own words what the corresponding Type I and II errors are in this context.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "Type I error: Convicting a person who, in reality, did not commit the crime.   \n",
    "Type II error: Acquitting a person who, in reality, committed the crime.\n",
    "\n",
    "See [this video](https://www.youtube.com/watch?v=7mE-K_w1v90&list=PLvxOuBpazmsNo893xlpXNfMzVpRBjDH67) a more detailed explanation about the difference between the two types of errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Given the context which type of error (I or II) do you believe is worse? Why?**\n",
    "\n",
    "**Solution:**  \n",
    "For this question there is not a good or wrong answer per se; it is dependent on your own ethical point of view. In general most people find a Type I error in this context (sentencing someone to jail who is innocent) worse though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 Assume that the court uses hypothesis testing to form its final verdict. What can we say about the significance level of this test if the court wants to minimize the number of Type I errors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "The definition of a Type I error is: rejecting the $H_0$ when in reality it is true (the shaded parts in the graph below). This rejection region can be decreased by increasing the critical value (e.g. $\\pm 1.96\\sigma$) which in turn is the effect of a smaller significance level ($\\alpha$). For example, the rejection region for $\\alpha=0.01$ is smaller than for $\\alpha=0.05$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![significance_level](../img/significance_level.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 Explain the consequences of choosing a significance level of 0.10 instead of 0.05 in this context. What will be the effect on the number of Type I and II errors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "In line with the previous question the number of Type I errors increases. The opposite holds for the number of Type II errors: as a result of moving the critical value closer to $\\mu$ we are less likely to fail to reject the null-hypothesis and thus the number of Type II errors decreases (basically it is a trade-off). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 Here you see an empty diagram with 3 axes: **\n",
    "\n",
    "![alpha_beta_power](../img/alpha_beta_power.jpg)\n",
    "- Type I error (horizontal - $\\alpha$)\n",
    "- Type II error (left hand side $\\beta$)\n",
    "- Power (right hand side)  \n",
    "\n",
    "**If you were to draw a line in this diagram how would it look like (in particular: what would the direction be)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "Power is defined as $1-\\beta$ which automatically follows from the two vertical axes (e.g. if $\\beta=1$ then the power is 0 and the other way around). The real question is whether there is a positive or negative relationship between $\\alpha$ and $\\beta$. Based on the previous question we know that if the number of Type I errors goes up as a result of changing the significance level the number of Type II errors goes down. As such, there is a negative relationship between $\\alpha$ and $\\beta$: a line from the top left corner to the bottom right corner. Note that, in reality, it's not a linear relationship that is solely dependent on the significance level (though that is outside the scope of this course; it suffices to know the direction of the line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Standard Deviation / Confidence Intervals\n",
    "Suppose two professsors (A and B) look into the grades for GA1. Professor A looks at a sample A of 10 DAE-students and professor B at a sample B of 100 DAE-students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 A student comments on the sample sizes of both professors: \"the standard deviation for A must be larger than for B because sample B contains 10 times as many data points\". Do you agree? Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "No, a larger sample size does not automatically mean a smaller standard deviation. The formula for the standard devation for a sample ($s$) is: \n",
    "$\\sqrt{\\frac{\\sum{(x-\\bar{x})^2}}{n-1}}$. You may be tempted to think that once $n$ increases the fraction decreases and thus $s$ decreases. However, we should not forget to take into account the actual values in our dataset ($x$). It could well be the case that for sample A all data points are very close to the mean ($\\bar{x}$) and thus $\\sum(x-\\bar{x})^2)$ would be relatively small. On the other hand, for sample B it could theoretically be the possible that half of the students did not show up and got a grade of 0/10 whereas the other half performed excellent and got a score of 10/10. Consequently, the sample mean ($\\bar{x}$) would be a 5 and the standard deviation would be huge. \n",
    "\n",
    "In short, it depends on the grades distribution; based on the information provided in this exercise we cannot draw any definite conclusion about the standard deviation of sample A relative to B (or vice versa). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 The professors find a 90% confidence interval of 6.9-8.3 for the GA1-grades. What can we conclude about the p-value for $\\alpha=0.10$ given the following two hypotheses: $H_0:\\mu=7.1$ and $H_a: \\mu \\neq 7.1?$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "Confidence intervals are computed as follows: $\\bar{X} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}$. In other words, the sample mean ($\\bar{X}$) plus or minus an additional fraction to account for variability. A 90% confidence interval corresponds to $\\alpha=0.10$. Hence, we know that if our hypothesized mean (i.e. 7.1) falls in the interval {6.9-8.3} we will not reject the null-hypothesis ($H_0$) and thus the p-value will always be greater than $\\alpha$ (0.10). See [this video](https://www.youtube.com/watch?v=k1at8VukIbw&list=PLvxOuBpazmsNo893xlpXNfMzVpRBjDH67) for a more in-depth explanation about the relationship between confidence intervals and hypothesis tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Binomial Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance ($\\sigma^2$) and mean ($\\mu$) for a binomial distribution can be computed as follows (where $p$ is the probability and $n$ the number of individuals/trials):   \n",
    "$\\sigma^2  = n\\dot p \\dot(1-p)$  \n",
    "$\\mu  = n\\dot p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1 Derive the formula for the standard deviation (sd) for a binomial distribution and determine for which value of p the sd is largest.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "The standard deviation ($\\sigma$) is simply the square root of the variance ($\\sigma^2$), so $\\sigma = \\sqrt{n\\dot p \\dot(1-p)}$. Since $n$ is a constant we only need to consider $p$ and thus can simply take the derivative of $p(1-p)$ which gives $p=\\frac{1}{2}$ (the derivative of $-p^2+p$ is $-2p+1$ and thus $2p=1$ or $p=\\frac{1}{2}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2 Explain in your own words why this value for p makes sense.**  \n",
    "*Note that p stands for the probability of success (something different than the \"p-value\" we often refer to in hypotheses testing)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "As its name suggests, every trial in a binomial distribution can either be a \"success\" or a \"failure\" (two/bi outcomes). An often used example for this distribution is therefore flipping a coin. For a fair coin holds that the chance of getting heads ($p=\\frac{1}{2}$) equals the chance of getting tails ($p=\\frac{1}{2}$). In this coin flipping example, it intuitively makes sense that you get most variation for a fair coin. In contrast, suppose that for a given coin the chance of getting heads or tails is 99% and 1% respectively. This would imply that - on average - we would only get tails 1 in a 100 times and thus there is much less variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6 - Association Rule Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to find out common relationships between disease A and B. Two metrics you should consider are support and confidence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Support = \\frac{\\text{Number of disease } A\\  \\cap\\  B}{\\text{Total number of disease}}$  \n",
    "\n",
    "$Confidence = \\frac{\\text{Number of disease } A\\  \\cap\\  B}{\\text{Number of disease A}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1 What is the effect of choosing a (too) low support cut-off level in association rule learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "In association rule learning we first pick a support cut-off level: in this context that is the minimal percentage or patients that should have both disease A and B (compared to the total number of patients). Now, consider the  numerical example below. Here we only look at the relationship between disease A and B, but there could be many more possible input variables (e.g. disease C, D, E .. etc.). The effect of choosing a too low support level is that you will get a result explosion: there are too many relations to consider while certain combinations only occur for a relatively small subset.\n",
    "![support](../img/support.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2 What is the effect of choosing a (too) high confidence cut-off level in association rule learning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution: **  \n",
    "The second step is to test for each possible relationship you found in step 1 whether it satisfies the confidence cut-off level. In this context: the percentage of patients with disease A that also have disease B. Again consider the example below. We find that of all patients with disease A 75% also has disease B. Suppose that the minimal confidence level would be 80%, then we would reject the relation A => B. So the consequence of picking a too high confidence cut-off level is that you will basically reject all possible association rules. \n",
    "\n",
    "![confidence_level](../img/confidence_level.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7 - Decision Tree Mining\n",
    "Three key metrics you should consider when evaluating the performance of a decision tree are the *accuracy*, *precision* and *recall*: \n",
    "\n",
    "* $Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}$ \n",
    "\n",
    "\n",
    "* $Recall = \\frac{TP}{TP+FN}$  \n",
    "\n",
    "\n",
    "* $Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "(TP = True positive, TN = True negative, FP = False positive, FN = False negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.1 A student argues that a high accuracy automatically means you have a high precision. Do you agree? Illustrate with an example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution: **  \n",
    "No, if the dataset is heavily skewed (i.e. there are many more nos than yesses or vice versa) this does not necessarily have to be the case. For example, see the numerical example below where the accuracy is 95% while the precision is only 8% (note that there are 303 nos but only 12 yesses). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![accuracy](../img/accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![precision](../img/precision.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8 - Mean Absolute Error / Root Mean Squared Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1 Two metrics used to measure accuracy for continuous variables are the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE).**  \n",
    "*Tip: https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**  \n",
    "As becomes clear from the example below the RMSE penalizes large errors relatively more than the MAE as a result of squaring the error term $(y_j-\\hat{y_j}^2)$. For example, the prediction for the 3rd record is 16 units off (19-3) which leads to a RMSE of 8.2 compared to a MAE of only 5.\n",
    "![MAE_RMSE](../img/MAE_RMSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last-minute questions? \n",
    "You are more than welcome to make use of our online discussion board on [AllAnswered](https://www.allanswered.com/community/s/data-analytics-for-engineers/). All the best with the final exam!   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
